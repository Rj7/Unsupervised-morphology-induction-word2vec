{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "import gensim, logging\n",
    "import nltk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "from multiprocessing import Process, Pool\n",
    "import os\n",
    "import collections\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.gutenberg.fileids()\n",
    "# total_corpus = []\n",
    "# for file in nltk.corpus.gutenberg.fileids():\n",
    "#     with open((nltk.corpus.gutenberg.words(file).fileid.path), 'r') as f:\n",
    "#         for line in f.readlines():\n",
    "#             try:\n",
    "#                 total_corpus.append(line.split())\n",
    "#             except:\n",
    "#                 print (\"hi\")\n",
    "#                 print (line)\n",
    "\n",
    "# with open(emma.fileid.path) as f:\n",
    "#     for line in f.readlines():\n",
    "#         print (line)\n",
    "\n",
    "# emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "# vocab = list(set(emma))\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns_in_words(patterns,pattern_counter,word1,word2,max_len):\n",
    "    i = 1\n",
    "    while(word1[:i] == word2[:i]):\n",
    "        i = i + 1\n",
    "    if i != 1 and i > max(len(word1[i-1:]), len(word2[i-1:])) < max_len:\n",
    "        pattern_counter[(\"suffix\",word1[i-1:], word2[i-1:])] += 1\n",
    "        if (\"suffix\",word1[i-1:], word2[i-1:]) in patterns:\n",
    "            patterns[(\"suffix\",word1[i-1:], word2[i-1:])].append((word1, word2))\n",
    "        else:\n",
    "            patterns[(\"suffix\",word1[i-1:], word2[i-1:])] = [(word1, word2)]\n",
    "#         patterns[(\"suffix\",word1[i-1:], word2[i-1:], word1, word2)] += 1\n",
    "    i = 1\n",
    "    while(word1[-i:] == word2[-i:]):\n",
    "        i = i + 1\n",
    "    if i != 1 and max(len(word1[:-i+1]), len(word2[:-i+1])) < max_len:\n",
    "        pattern_counter[(\"prefix\",word1[:-i+1], word2[:-i+1])] += 1\n",
    "        if (\"prefix\",word1[:-i+1], word2[:-i+1]) in patterns:\n",
    "            patterns[(\"prefix\",word1[:-i+1], word2[:-i+1])].append((word1, word2))\n",
    "        else:\n",
    "            patterns[(\"prefix\",word1[:-i+1], word2[:-i+1])] = [(word1, word2)]\n",
    "#         patterns[(\"prefix\",word1[:-i+1], word2[:-i+1], word1, word2)] += 1\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_dict(vocab,max_len = 6):\n",
    "    patterns  = defaultdict(list)\n",
    "    print (patterns)\n",
    "    pattern_counter = Counter()\n",
    "    for word in vocab:\n",
    "        for second_word in vocab:\n",
    "            if word != second_word:\n",
    "                extract_patterns_in_words(patterns,pattern_counter,word,second_word,max_len)\n",
    "    return patterns, pattern_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_patterns():\n",
    "    #Downsample to include only top 1000\n",
    "    pattern_1000 = defaultdict(list)\n",
    "    for pattern,items in patterns.items():\n",
    "        shuffle(items)\n",
    "        pattern_1000[pattern] = items[:1000]\n",
    "    return pattern_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_wise_similarity(word_pair1, word_pair2,topn = 10):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair1[0], word_pair2[0]], negative=[word_pair2[1]], topn=topn)\n",
    "#     print (word_pair1, word_pair2)\n",
    "#     print (closest_n)\n",
    "    for word, cos_sim in closest_n:\n",
    "        if word == word_pair2[1]:\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vector(word_vectors, dimensions=300):\n",
    "    fname = '../data/annoy.index'\n",
    "    # Persist index to disk\n",
    "    if os.path.exists(fname):\n",
    "        annoy_index = AnnoyIndexer()\n",
    "        annoy_index.load(fname)\n",
    "        annoy_index.model = word_vectors\n",
    "    else:\n",
    "        annoy_index = AnnoyIndexer(word_vectors, dimensions)\n",
    "    annoy_index.save(fname)\n",
    "    return annoy_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annoy_pair_wise_similarity(word_pair1, word_pair2,topn = 10):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair1[0], word_pair2[0]], negative=[word_pair2[1]], topn=topn, indexer=annoy_index)\n",
    "#     print (word_pair1, word_pair2)\n",
    "#     print (closest_n)\n",
    "    for word, cos_sim in closest_n:\n",
    "        if word == word_pair2[1]:\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(patterns, word_vectors, similarity_function):\n",
    "    hit_rates = {}\n",
    "    for (pattern,support_set) in patterns.items():\n",
    "        hit_count = 0\n",
    "        for pair1 in support_set:\n",
    "            for pair2 in support_set:\n",
    "                if pair1 != pair2 and similarity_function(pair1, pair2, 20):\n",
    "                    hit_count += 1\n",
    "        hit_rates[pattern] = hit_count / float(len(support_set))\n",
    "    return hit_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Common Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern_counter.most_common()[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 260 ms, sys: 0 ns, total: 260 ms\n",
      "Wall time: 259 ms\n"
     ]
    }
   ],
   "source": [
    "%time word_vectors = KeyedVectors.load_word2vec_format('/home/raja/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {})\n",
      "CPU times: user 21.8 s, sys: 20 ms, total: 21.8 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%time patterns, pattern_counter = build_pattern_dict(word_vectors.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('suffix', 's', ''), 598),\n",
       " (('suffix', '', 's'), 598),\n",
       " (('suffix', 'ed', ''), 168),\n",
       " (('suffix', '', 'ed'), 168),\n",
       " (('suffix', '', 'ing'), 149),\n",
       " (('suffix', 'ing', ''), 149),\n",
       " (('suffix', 'ing', 'ed'), 141),\n",
       " (('suffix', 'ed', 'ing'), 141),\n",
       " (('suffix', '', 'd'), 127),\n",
       " (('suffix', 'd', ''), 127),\n",
       " (('suffix', 'ed', 's'), 86),\n",
       " (('suffix', 's', 'ed'), 86),\n",
       " (('suffix', 'ly', ''), 80),\n",
       " (('suffix', '', 'ly'), 80),\n",
       " (('suffix', 's', 'ing'), 76),\n",
       " (('suffix', 'ing', 's'), 76),\n",
       " (('suffix', 'ing', 'e'), 74),\n",
       " (('suffix', 'e', 'ing'), 74),\n",
       " (('suffix', 's', 'd'), 62),\n",
       " (('suffix', 'd', 's'), 62)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 308 ms, total: 1.66 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%time annoy_index = index_vector(word_vectors=word_vectors, dimensions=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.36 s, sys: 0 ns, total: 3.36 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%time sampled_patterns = downsample_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time hit_rates = calculate_hit_rate(sampled_patterns, word_vectors, pair_wise_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "type(hit_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = Counter(hit_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_patterns[('prefix', 'hai', 'close')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "import gensim, logging\n",
    "import nltk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "from multiprocessing import Process, Pool\n",
    "import os\n",
    "import collections\n",
    "from random import shuffle\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.gutenberg.fileids()\n",
    "# total_corpus = []\n",
    "# for file in nltk.corpus.gutenberg.fileids():\n",
    "#     with open((nltk.corpus.gutenberg.words(file).fileid.path), 'r') as f:\n",
    "#         for line in f.readlines():\n",
    "#             try:\n",
    "#                 total_corpus.append(line.split())\n",
    "#             except:\n",
    "#                 print (\"hi\")\n",
    "#                 print (line)\n",
    "\n",
    "# with open(emma.fileid.path) as f:\n",
    "#     for line in f.readlines():\n",
    "#         print (line)\n",
    "\n",
    "# emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "# vocab = list(set(emma))\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns_in_words(patterns,pattern_counter,word1,word2,max_len):\n",
    "    i = 1\n",
    "    while(word1[:i] == word2[:i]):\n",
    "        i = i + 1\n",
    "    if i != 1 and i > max(len(word1[i-1:]), len(word2[i-1:])) < max_len:\n",
    "        pattern_counter[(\"suffix\",word1[i-1:], word2[i-1:])] += 1\n",
    "        if (\"suffix\",word1[i-1:], word2[i-1:]) in patterns:\n",
    "            patterns[(\"suffix\",word1[i-1:], word2[i-1:])].append((word1, word2))\n",
    "        else:\n",
    "            patterns[(\"suffix\",word1[i-1:], word2[i-1:])] = [(word1, word2)]\n",
    "#         patterns[(\"suffix\",word1[i-1:], word2[i-1:], word1, word2)] += 1\n",
    "    i = 1\n",
    "    while(word1[-i:] == word2[-i:]):\n",
    "        i = i + 1\n",
    "    if i != 1 and max(len(word1[:-i+1]), len(word2[:-i+1])) < max_len:\n",
    "        pattern_counter[(\"prefix\",word1[:-i+1], word2[:-i+1])] += 1\n",
    "        if (\"prefix\",word1[:-i+1], word2[:-i+1]) in patterns:\n",
    "            patterns[(\"prefix\",word1[:-i+1], word2[:-i+1])].append((word1, word2))\n",
    "        else:\n",
    "            patterns[(\"prefix\",word1[:-i+1], word2[:-i+1])] = [(word1, word2)]\n",
    "#         patterns[(\"prefix\",word1[:-i+1], word2[:-i+1], word1, word2)] += 1\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_dict(vocab,max_len = 6):\n",
    "    if os.path.exists('../data/patterns_'+ str(len(vocab))):\n",
    "        patterns_file_r = open('../data/patterns_'+ str(len(vocab)), 'rb')\n",
    "        pattern_counter_file_r = open('../data/patterns_counter_'+ str(len(vocab)), 'rb')\n",
    "        patterns = pickle.load(patterns_file_r)\n",
    "        pattern_counter = pickle.load(pattern_counter_file_r)\n",
    "    else:\n",
    "        patterns_file_w = open('../data/patterns_'+ str(len(vocab)),\"wb\" )\n",
    "        pattern_counter_file_w = open('../data/patterns_counter_'+ str(len(vocab)),\"wb\" )\n",
    "        patterns  = defaultdict(list)\n",
    "#         print (patterns)\n",
    "        pattern_counter = Counter()\n",
    "        for word in vocab:\n",
    "            for second_word in vocab:\n",
    "                if word != second_word:\n",
    "                    extract_patterns_in_words(patterns,pattern_counter,word,second_word,max_len)\n",
    "        pickle.dump(patterns, patterns_file_w)\n",
    "        patterns_file_w.close()\n",
    "        pickle.dump(pattern_counter, pattern_counter_file_w)\n",
    "        pattern_counter_file_w.close()\n",
    "    return patterns, pattern_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_patterns():\n",
    "    #Downsample to include only top 1000\n",
    "    pattern_1000 = defaultdict(list)\n",
    "    for pattern,items in patterns.items():\n",
    "        shuffle(items)\n",
    "        pattern_1000[pattern] = items[:1000]\n",
    "    return pattern_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_wise_similarity(word_pair1, word_pair2,annoy_index=None, topn = 10,):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]], topn=topn)\n",
    "#     print (word_pair2[1])\n",
    "#     print (closest_n)\n",
    "    for word, cos_sim in closest_n:\n",
    "        if word == word_pair2[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def annoy_pair_wise_similarity(word_pair1, word_pair2,annoy_index, topn = 10):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]], topn=topn, indexer=annoy_index)\n",
    "#     print (word_pair2[1])\n",
    "#     print (closest_n)\n",
    "    for word, cos_sim in closest_n:\n",
    "        if word == word_pair2[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_similarity_rank(word_pair1, word_pair2, topn=500):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]], topn=topn)\n",
    "#     print (word_pair2[1])\n",
    "#     print (closest_n)\n",
    "    for n,(word, cos_sim) in enumerate(closest_n):\n",
    "        if word == word_pair2[1]:\n",
    "            return (n, cos_sim)\n",
    "    return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vector(word_vectors, dimensions=300):\n",
    "    fname = '../data/annoy.index'\n",
    "    # Persist index to disk\n",
    "    if os.path.exists(fname):\n",
    "        annoy_index = AnnoyIndexer()\n",
    "        annoy_index.load(fname)\n",
    "        annoy_index.model = word_vectors\n",
    "    else:\n",
    "        annoy_index = AnnoyIndexer(word_vectors, dimensions)\n",
    "        annoy_index.save(fname)\n",
    "    return annoy_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(patterns, similarity_function):\n",
    "    if os.path.exists('../data/hitrate_'+ str(len(word_vectors.vocab))):\n",
    "        hit_rate_file_r = open('../data/hitrate_'+ str(len(word_vectors.vocab)), 'rb')\n",
    "        hit_rates_rules = pickle.load(hit_rate_file_r)\n",
    "        return hit_rates_rules\n",
    "    else:\n",
    "        hit_rate_file_w = open('../data/hitrate_'+ str(len(word_vectors.vocab)),\"wb\" )\n",
    "        hit_rates_rules = {}\n",
    "        for (pattern,support_set) in patterns.items():\n",
    "            hit_rates_word_pair = {}\n",
    "            for pair1 in support_set:\n",
    "                hit_count = 0\n",
    "                hit_pairs = set()\n",
    "                for pair2 in support_set:\n",
    "                    if pair1 != pair2 and similarity_function(pair1, pair2, 10):\n",
    "                        hit_count += 1\n",
    "                        hit_pairs.add(pair2)\n",
    "                if len(support_set) ==1:\n",
    "                    total = 1\n",
    "                else:\n",
    "                    total = len(support_set) - 1\n",
    "                if hit_count != 0:\n",
    "                    hit_rates_word_pair[pair1] =  hit_pairs\n",
    "            if len(support_set) != 1 and hit_rates_word_pair:\n",
    "                hit_rates_rules[pattern] = hit_rates_word_pair\n",
    "        pickle.dump(hit_rates_rules, hit_rate_file_w)\n",
    "        hit_rate_file_w.close()\n",
    "        return hit_rates_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern_counter.most_common()[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 400 ms, sys: 8 ms, total: 408 ms\n",
      "Wall time: 408 ms\n"
     ]
    }
   ],
   "source": [
    "%time word_vectors = KeyedVectors.load_word2vec_format('/home/raja/models/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 1.68 s, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%time patterns, pattern_counter = build_pattern_dict(word_vectors.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('suffix', 's', ''), 1328),\n",
       " (('suffix', '', 's'), 1328),\n",
       " (('suffix', 'ing', 'ed'), 368),\n",
       " (('suffix', 'ed', 'ing'), 368),\n",
       " (('suffix', 'ed', ''), 341),\n",
       " (('suffix', '', 'ed'), 341),\n",
       " (('suffix', '', 'ing'), 330),\n",
       " (('suffix', 'ing', ''), 330),\n",
       " (('suffix', '', 'd'), 258),\n",
       " (('suffix', 'd', ''), 258),\n",
       " (('suffix', 'ing', 's'), 196),\n",
       " (('suffix', 's', 'ing'), 196),\n",
       " (('suffix', 'ed', 's'), 190),\n",
       " (('suffix', 's', 'ed'), 190),\n",
       " (('suffix', 'ing', 'e'), 177),\n",
       " (('suffix', 'e', 'ing'), 177),\n",
       " (('suffix', 's', 'd'), 174),\n",
       " (('suffix', 'd', 's'), 174),\n",
       " (('suffix', 'ly', ''), 166),\n",
       " (('suffix', '', 'ly'), 166)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.35 s, sys: 184 ms, total: 9.54 s\n",
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "%time sampled_patterns = downsample_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_vectors_annoy = deepcopy(word_vectors)\n",
    "# word_vectors.init_sims(replace=True)\n",
    "# %time annoy_index = index_vector(word_vectors=word_vectors, dimensions=300)\n",
    "# %time hit_rates = calculate_hit_rate(sampled_patterns, annoy_pair_wise_similarity, annoy_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair_wise_similarity((\"asking\", \"banking\"), (\"ask\", \"bank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.31 s, sys: 292 ms, total: 3.6 s\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%time hit_rates = get_hit_rate(sampled_patterns, pair_wise_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temp(pattern, transformations):\n",
    "# #     ll = [transformations[transformation] for transformation in transformations]\n",
    "# #     print (ll)\n",
    "# #     print (len(ll), len(transformations))\n",
    "#     print (transformations)\n",
    "#     prototypes = []\n",
    "#     support_set = set(sampled_patterns[pattern])\n",
    "# #     print (pattern)\n",
    "# #     print (support_set)\n",
    "#     while True:\n",
    "#         explains_by_count = sorted(transformations.items(), key=lambda kv: -len(kv[1]))\n",
    "#         print (explains_by_count)\n",
    "#         best = explains_by_count[0]\n",
    "#         if len(best[1]) >= 10: #The prototype explains more than 10 word pairs\n",
    "#             prototypes.append((best[0][0], len(best[1]) / float(len(support_set))))\n",
    "#         else:\n",
    "#             break\n",
    "#         del transformations[best[0]]\n",
    "#         #Remove all explained pairs from the support set\n",
    "#         support_set = support_set - best[1]\n",
    "#         for k, v in transformations.items():\n",
    "#             transformations[k] = transformations[k] - best[1]\n",
    "#         explains_by_count.pop(0)\n",
    "\n",
    "#         if not (len(support_set) >= 10 and len(explains_by_count) and explains_by_count[0] >= 10):\n",
    "#             break\n",
    "#     print (prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hit_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = 4\n",
    "def update_morpho_rules(patterns):\n",
    "    global morphological_rules\n",
    "    for pattern in patterns:\n",
    "        transformations = patterns[pattern]\n",
    "#         ll = [transformations[transformation] for transformation in transformations]\n",
    "    #     print (ll)\n",
    "    #     print (len(ll), len(transformations))\n",
    "        support_set = set(sampled_patterns[pattern])\n",
    "        while(True):\n",
    "            transformations_by_count = sorted(transformations.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "            best = transformations_by_count[0]\n",
    "    #         print (transformations_by_count)\n",
    "    #         print (transformations)\n",
    "            if len(best[1]) >= cons:\n",
    "                morphological_rules[best[0]] = (pattern, len(best[1]) / float(len(support_set)),  best[1])\n",
    "    #             directions.append(best)\n",
    "                del transformations[best[0]]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            #Remove all explained pairs from the support set\n",
    "            #TODO: Remove best[0] from support set and transformations\n",
    "            support_set = support_set - best[1]\n",
    "            for k, v in transformations.items():\n",
    "    #             print (\"*\"*50)\n",
    "    #             print (transformations[k])\n",
    "                transformations[k] = transformations[k] - best[1]\n",
    "    #             print (transformations[k])\n",
    "    #             print (\"__\"*50)\n",
    "\n",
    "            transformations_by_count.pop(0)\n",
    "            if not (len(support_set) >= cons and len(transformations_by_count) and len(transformations_by_count[0][1]) >= cons):\n",
    "                break\n",
    "    #     return directions\n",
    "    #         print (best_transformation,best_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time patterns = calculate_hit_rate(sampled_patterns, pair_wise_similarity)\n",
    "morphological_rules = {}\n",
    "%time update_morpho_rules(hit_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(morphological_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphological_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in morphological_rules:\n",
    "    print (item, morphological_rules[item])\n",
    "    print (\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rule in morphological_rules:\n",
    "    print (rule)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists('../data/rules_'+ str(len(word_vectors.vocab))):\n",
    "#     rule_file_r = open('../data/rules_'+ str(len(word_vectors.vocab)), 'rb')\n",
    "#     morphological_rules = pickle.load(rule_file_r)\n",
    "# else:\n",
    "#     hit_rates = calculate_hit_rate(sampled_patterns, pair_wise_similarity)\n",
    "#     hit_rate_file_w = open('../data/rules_'+ str(len(word_vectors.vocab)),\"wb\" )\n",
    "#     pickle.dump(hit_rates, hit_rate_file_w)\n",
    "#     hit_rate_file_w.close()\n",
    "# # %time hit_rates = calculate_hit_rate(sampled_patterns, pair_wise_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time G.add_nodes_from(word_vectors.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dw,support in morphological_rules.items():\n",
    "    morp_rule, hit_rate,support_set = support\n",
    "    (word1, word2) = dw\n",
    "    for (word3, word4) in support_set:\n",
    "        (rank,cos_sim) = get_similarity_rank((word1,word2),(word3,word4))\n",
    "        if rank < 3 and cos_sim > 0.5:\n",
    "            if not G.has_edge(word3,word4,key=dw):\n",
    "                G.add_edge(word3,word4,key=dw,cos=cos_sim,rank=rank)\n",
    "        else:\n",
    "#             print (rank,cos_sim, word3, word2, dw)\n",
    "            pass\n",
    "\n",
    "#         G.add_edge(word1, word2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in G['make']:\n",
    "    print (item)\n",
    "    d = G['make'][item]\n",
    "    print(d)\n",
    "print (G.out_edges('make',data=True))\n",
    "print (G.out_edges('make',keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(G.edges))\n",
    "print (len(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in G.nodes:\n",
    "    if G.out_degree(item) > 1:\n",
    "        print (item, G.out_degree(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in list(G.nodes):\n",
    "    for neighbor in list(G.neighbors(node)):\n",
    "        if word_vectors.vocab[node].count > word_vectors.vocab[neighbor].count:\n",
    "            if G.has_edge(neighbor, node):\n",
    "                G.remove_edge(neighbor, node)\n",
    "        else:\n",
    "            if G.has_edge(node, neighbor):\n",
    "                G.remove_edge(node, neighbor)\n",
    "                print (G.number_of_edges(node,neighbor))\n",
    "        if G.number_of_edges(node,neighbor) > 1:\n",
    "            print (\"AHHH\", neighbor, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_edge_data('reported', 'started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_edge_data('started','reported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(G.nodes))\n",
    "print (len(G.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in G.edges():\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = nx.MultiGraph()\n",
    "M.add_edge(1,2,weight=19)\n",
    "M.add_edge(1,2,weight=7)\n",
    "M.add_edge(2,3,weight=42)\n",
    "\n",
    "G2 = nx.Graph(M,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

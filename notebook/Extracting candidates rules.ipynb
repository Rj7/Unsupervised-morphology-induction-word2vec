{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "import gensim, logging\n",
    "import nltk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "from multiprocessing import Process, Pool\n",
    "import os\n",
    "import collections\n",
    "from random import shuffle\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.gutenberg.fileids()\n",
    "# total_corpus = []\n",
    "# for file in nltk.corpus.gutenberg.fileids():\n",
    "#     with open((nltk.corpus.gutenberg.words(file).fileid.path), 'r') as f:\n",
    "#         for line in f.readlines():\n",
    "#             try:\n",
    "#                 total_corpus.append(line.split())\n",
    "#             except:\n",
    "#                 print (\"hi\")\n",
    "#                 print (line)\n",
    "\n",
    "# with open(emma.fileid.path) as f:\n",
    "#     for line in f.readlines():\n",
    "#         print (line)\n",
    "\n",
    "# emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "# vocab = list(set(emma))\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns_in_words(patterns,pattern_counter,word1,word2,max_len):\n",
    "    i = 1\n",
    "    while(word1[:i] == word2[:i]):\n",
    "        i = i + 1\n",
    "    if i != 1 and i > max(len(word1[i-1:]), len(word2[i-1:])) < max_len:\n",
    "        pattern_counter[(\"suffix\",word1[i-1:], word2[i-1:])] += 1\n",
    "        if (\"suffix\",word1[i-1:], word2[i-1:]) in patterns:\n",
    "            patterns[(\"suffix\",word1[i-1:], word2[i-1:])].append((word1, word2))\n",
    "        else:\n",
    "            patterns[(\"suffix\",word1[i-1:], word2[i-1:])] = [(word1, word2)]\n",
    "#         patterns[(\"suffix\",word1[i-1:], word2[i-1:], word1, word2)] += 1\n",
    "    i = 1\n",
    "    while(word1[-i:] == word2[-i:]):\n",
    "        i = i + 1\n",
    "    if i != 1 and max(len(word1[:-i+1]), len(word2[:-i+1])) < max_len:\n",
    "        pattern_counter[(\"prefix\",word1[:-i+1], word2[:-i+1])] += 1\n",
    "        if (\"prefix\",word1[:-i+1], word2[:-i+1]) in patterns:\n",
    "            patterns[(\"prefix\",word1[:-i+1], word2[:-i+1])].append((word1, word2))\n",
    "        else:\n",
    "            patterns[(\"prefix\",word1[:-i+1], word2[:-i+1])] = [(word1, word2)]\n",
    "#         patterns[(\"prefix\",word1[:-i+1], word2[:-i+1], word1, word2)] += 1\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_dict(vocab,max_len = 6):\n",
    "    if os.path.exists('../data/patterns_'+ str(len(vocab))):\n",
    "        patterns_file_r = open('../data/patterns_'+ str(len(vocab)), 'rb')\n",
    "        pattern_counter_file_r = open('../data/patterns_counter_'+ str(len(vocab)), 'rb')\n",
    "        patterns = pickle.load(patterns_file_r)\n",
    "        pattern_counter = pickle.load(pattern_counter_file_r)\n",
    "    else:\n",
    "        patterns_file_w = open('../data/patterns_'+ str(len(vocab)),\"wb\" )\n",
    "        pattern_counter_file_w = open('../data/patterns_counter_'+ str(len(vocab)),\"wb\" )\n",
    "        patterns  = defaultdict(list)\n",
    "#         print (patterns)\n",
    "        pattern_counter = Counter()\n",
    "        for word in vocab:\n",
    "            for second_word in vocab:\n",
    "                if word != second_word:\n",
    "                    extract_patterns_in_words(patterns,pattern_counter,word,second_word,max_len)\n",
    "        pickle.dump(patterns, patterns_file_w)\n",
    "        patterns_file_w.close()\n",
    "        pickle.dump(pattern_counter, pattern_counter_file_w)\n",
    "        pattern_counter_file_w.close()\n",
    "    return patterns, pattern_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_patterns():\n",
    "    #Downsample to include only top 1000\n",
    "    pattern_1000 = defaultdict(list)\n",
    "    for pattern,items in patterns.items():\n",
    "        shuffle(items)\n",
    "        pattern_1000[pattern] = items[:1000]\n",
    "    return pattern_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_wise_similarity(word_pair1, word_pair2,annoy_index=None, topn = 10,):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]], topn=topn)\n",
    "#     print (word_pair2[1])\n",
    "#     print (closest_n)\n",
    "    for word, cos_sim in closest_n:\n",
    "        if word == word_pair2[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def annoy_pair_wise_similarity(word_pair1, word_pair2,annoy_index, topn = 10):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]], topn=topn, indexer=annoy_index)\n",
    "#     print (word_pair2[1])\n",
    "#     print (closest_n)\n",
    "    for word, cos_sim in closest_n:\n",
    "        if word == word_pair2[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_similarity_rank(word_pair1, word_pair2, topn=500):\n",
    "    closest_n = word_vectors.most_similar(positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]], topn=topn)\n",
    "#     print (word_pair2[1])\n",
    "#     print (closest_n)\n",
    "    for n,(word, cos_sim) in enumerate(closest_n):\n",
    "        if word == word_pair2[1]:\n",
    "            return (n, cos_sim)\n",
    "    return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vector(word_vectors, dimensions=300):\n",
    "    fname = '../data/annoy.index'\n",
    "    # Persist index to disk\n",
    "    if os.path.exists(fname):\n",
    "        annoy_index = AnnoyIndexer()\n",
    "        annoy_index.load(fname)\n",
    "        annoy_index.model = word_vectors\n",
    "    else:\n",
    "        annoy_index = AnnoyIndexer(word_vectors, dimensions)\n",
    "        annoy_index.save(fname)\n",
    "    return annoy_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(patterns, similarity_function):\n",
    "    if os.path.exists('../data/hitrate_'+ str(len(word_vectors.vocab))):\n",
    "        hit_rate_file_r = open('../data/hitrate_'+ str(len(word_vectors.vocab)), 'rb')\n",
    "        hit_rates_rules = pickle.load(hit_rate_file_r)\n",
    "        return hit_rates_rules\n",
    "    else:\n",
    "        hit_rate_file_w = open('../data/hitrate_'+ str(len(word_vectors.vocab)),\"wb\" )\n",
    "        hit_rates_rules = {}\n",
    "        for (pattern,support_set) in patterns.items():\n",
    "            hit_rates_word_pair = {}\n",
    "            for pair1 in support_set:\n",
    "                hit_count = 0\n",
    "                hit_pairs = set()\n",
    "                for pair2 in support_set:\n",
    "                    if pair1 != pair2 and similarity_function(pair1, pair2, 10):\n",
    "                        hit_count += 1\n",
    "                        hit_pairs.add(pair2)\n",
    "                if len(support_set) ==1:\n",
    "                    total = 1\n",
    "                else:\n",
    "                    total = len(support_set) - 1\n",
    "                if hit_count != 0:\n",
    "                    hit_rates_word_pair[pair1] =  hit_pairs\n",
    "            if len(support_set) != 1 and hit_rates_word_pair:\n",
    "                hit_rates_rules[pattern] = hit_rates_word_pair\n",
    "        pickle.dump(hit_rates_rules, hit_rate_file_w)\n",
    "        hit_rate_file_w.close()\n",
    "        return hit_rates_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern_counter.most_common()[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 ms, sys: 4 ms, total: 44 ms\n",
      "Wall time: 51.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time word_vectors = KeyedVectors.load_word2vec_format('/home/raja/models/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 260 ms, sys: 36 ms, total: 296 ms\n",
      "Wall time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "%time patterns, pattern_counter = build_pattern_dict(word_vectors.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('suffix', 's', ''), 69),\n",
       " (('suffix', '', 's'), 69),\n",
       " (('suffix', '', 'ed'), 16),\n",
       " (('suffix', 'ed', ''), 16),\n",
       " (('suffix', '', 'd'), 13),\n",
       " (('suffix', 'd', ''), 13),\n",
       " (('suffix', 'n', ''), 11),\n",
       " (('suffix', '', 'n'), 11),\n",
       " (('suffix', 'ing', ''), 10),\n",
       " (('suffix', '', 'ing'), 10),\n",
       " (('suffix', '', 'er'), 8),\n",
       " (('prefix', '#', ''), 8),\n",
       " (('suffix', 'ing', 'e'), 8),\n",
       " (('suffix', 'er', ''), 8),\n",
       " (('prefix', '', '#'), 8),\n",
       " (('suffix', 'e', 'ing'), 8),\n",
       " (('prefix', 't', 'T'), 7),\n",
       " (('prefix', 'T', 't'), 7),\n",
       " (('suffix', '', 't'), 7),\n",
       " (('suffix', 't', ''), 7)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 ms, sys: 36 ms, total: 316 ms\n",
      "Wall time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "%time sampled_patterns = downsample_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_vectors_annoy = deepcopy(word_vectors)\n",
    "# word_vectors.init_sims(replace=True)\n",
    "# %time annoy_index = index_vector(word_vectors=word_vectors, dimensions=300)\n",
    "# %time hit_rates = calculate_hit_rate(sampled_patterns, annoy_pair_wise_similarity, annoy_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair_wise_similarity((\"asking\", \"banking\"), (\"ask\", \"bank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68 ms, sys: 4 ms, total: 72 ms\n",
      "Wall time: 74.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time hit_rates = get_hit_rate(sampled_patterns, pair_wise_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temp(pattern, transformations):\n",
    "# #     ll = [transformations[transformation] for transformation in transformations]\n",
    "# #     print (ll)\n",
    "# #     print (len(ll), len(transformations))\n",
    "#     print (transformations)\n",
    "#     prototypes = []\n",
    "#     support_set = set(sampled_patterns[pattern])\n",
    "# #     print (pattern)\n",
    "# #     print (support_set)\n",
    "#     while True:\n",
    "#         explains_by_count = sorted(transformations.items(), key=lambda kv: -len(kv[1]))\n",
    "#         print (explains_by_count)\n",
    "#         best = explains_by_count[0]\n",
    "#         if len(best[1]) >= 10: #The prototype explains more than 10 word pairs\n",
    "#             prototypes.append((best[0][0], len(best[1]) / float(len(support_set))))\n",
    "#         else:\n",
    "#             break\n",
    "#         del transformations[best[0]]\n",
    "#         #Remove all explained pairs from the support set\n",
    "#         support_set = support_set - best[1]\n",
    "#         for k, v in transformations.items():\n",
    "#             transformations[k] = transformations[k] - best[1]\n",
    "#         explains_by_count.pop(0)\n",
    "\n",
    "#         if not (len(support_set) >= 10 and len(explains_by_count) and explains_by_count[0] >= 10):\n",
    "#             break\n",
    "#     print (prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hit_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = 4\n",
    "def update_morpho_rules(patterns, morphological_rules):\n",
    "    for pattern in patterns:\n",
    "        transformations = patterns[pattern]\n",
    "#         ll = [transformations[transformation] for transformation in transformations]\n",
    "    #     print (ll)\n",
    "    #     print (len(ll), len(transformations))\n",
    "        support_set = set(sampled_patterns[pattern])\n",
    "        while(True):\n",
    "            transformations_by_count = sorted(transformations.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "            best = transformations_by_count[0]\n",
    "    #         print (transformations_by_count)\n",
    "    #         print (transformations)\n",
    "            if len(best[1]) >= cons:\n",
    "                morphological_rules[best[0]] = (pattern, len(best[1]) / float(len(support_set)),  best[1])\n",
    "    #             directions.append(best)\n",
    "                del transformations[best[0]]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            #Remove all explained pairs from the support set\n",
    "            #TODO: Remove best[0] from support set and transformations\n",
    "            support_set = support_set - best[1]\n",
    "            for k, v in transformations.items():\n",
    "    #             print (\"*\"*50)\n",
    "    #             print (transformations[k])\n",
    "                transformations[k] = transformations[k] - best[1]\n",
    "    #             print (transformations[k])\n",
    "    #             print (\"__\"*50)\n",
    "\n",
    "            transformations_by_count.pop(0)\n",
    "            if not (len(support_set) >= cons and len(transformations_by_count) and len(transformations_by_count[0][1]) >= cons):\n",
    "                break\n",
    "    return morphological_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 7.32 ms\n"
     ]
    }
   ],
   "source": [
    "#%time patterns = calculate_hit_rate(sampled_patterns, pair_wise_similarity)\n",
    "morphological_rules = {}\n",
    "%time morphological_rules = update_morpho_rules(hit_rates,morphological_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print (len(morphological_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('####', '###'): (('prefix', '#', ''),\n",
       "  0.875,\n",
       "  {('###', '##'),\n",
       "   ('###,###', '##,###'),\n",
       "   ('##,###', '#,###'),\n",
       "   ('##-#', '#-#'),\n",
       "   ('##.#', '#.#'),\n",
       "   ('##.##', '#.##'),\n",
       "   ('##:##', '#:##')}),\n",
       " ('#-#', '##-#'): (('prefix', '', '#'),\n",
       "  0.75,\n",
       "  {('##', '###'),\n",
       "   ('##,###', '###,###'),\n",
       "   ('#,###', '##,###'),\n",
       "   ('#.#', '##.#'),\n",
       "   ('#.##', '##.##'),\n",
       "   ('#:##', '##:##')}),\n",
       " ('According', 'according'): (('prefix', 'A', 'a'),\n",
       "  0.8333333333333334,\n",
       "  {('After', 'after'),\n",
       "   ('All', 'all'),\n",
       "   ('An', 'an'),\n",
       "   ('As', 'as'),\n",
       "   ('At', 'at')}),\n",
       " ('News', 'news'): (('prefix', 'N', 'n'),\n",
       "  0.8,\n",
       "  {('National', 'national'), ('New', 'new'), ('No', 'no'), ('Now', 'now')}),\n",
       " ('She', 'she'): (('prefix', 'S', 's'),\n",
       "  0.8,\n",
       "  {('School', 'school'), ('So', 'so'), ('Some', 'some'), ('State', 'state')}),\n",
       " ('This', 'this'): (('prefix', 'T', 't'),\n",
       "  0.8571428571428571,\n",
       "  {('That', 'that'),\n",
       "   ('The', 'the'),\n",
       "   ('There', 'there'),\n",
       "   ('These', 'these'),\n",
       "   ('They', 'they'),\n",
       "   ('Two', 'two')}),\n",
       " ('When', 'when'): (('prefix', 'W', 'w'),\n",
       "  0.8333333333333334,\n",
       "  {('We', 'we'),\n",
       "   ('What', 'what'),\n",
       "   ('While', 'while'),\n",
       "   ('With', 'with'),\n",
       "   ('World', 'world')}),\n",
       " ('according', 'According'): (('prefix', 'a', 'A'),\n",
       "  0.8333333333333334,\n",
       "  {('after', 'After'),\n",
       "   ('all', 'All'),\n",
       "   ('an', 'An'),\n",
       "   ('as', 'As'),\n",
       "   ('at', 'At')}),\n",
       " ('base', 'based'): (('suffix', '', 'd'),\n",
       "  0.8461538461538461,\n",
       "  {('charge', 'charged'),\n",
       "   ('close', 'closed'),\n",
       "   ('continue', 'continued'),\n",
       "   ('include', 'included'),\n",
       "   ('increase', 'increased'),\n",
       "   ('move', 'moved'),\n",
       "   ('name', 'named'),\n",
       "   ('provide', 'provided'),\n",
       "   ('receive', 'received'),\n",
       "   ('release', 'released'),\n",
       "   ('use', 'used')}),\n",
       " ('based', 'base'): (('suffix', 'd', ''),\n",
       "  0.8461538461538461,\n",
       "  {('charged', 'charge'),\n",
       "   ('closed', 'close'),\n",
       "   ('continued', 'continue'),\n",
       "   ('included', 'include'),\n",
       "   ('increased', 'increase'),\n",
       "   ('moved', 'move'),\n",
       "   ('named', 'name'),\n",
       "   ('provided', 'provide'),\n",
       "   ('received', 'receive'),\n",
       "   ('released', 'release'),\n",
       "   ('used', 'use')}),\n",
       " ('give', 'giving'): (('suffix', 'e', 'ing'),\n",
       "  0.875,\n",
       "  {('come', 'coming'),\n",
       "   ('have', 'having'),\n",
       "   ('include', 'including'),\n",
       "   ('live', 'living'),\n",
       "   ('make', 'making'),\n",
       "   ('take', 'taking'),\n",
       "   ('trade', 'trading')}),\n",
       " ('including', 'included'): (('suffix', 'ing', 'ed'),\n",
       "  0.8,\n",
       "  {('following', 'followed'),\n",
       "   ('playing', 'played'),\n",
       "   ('starting', 'started'),\n",
       "   ('working', 'worked')}),\n",
       " ('including', 'includes'): (('suffix', 'ing', 'es'),\n",
       "  0.8,\n",
       "  {('coming', 'comes'),\n",
       "   ('living', 'lives'),\n",
       "   ('making', 'makes'),\n",
       "   ('taking', 'takes')}),\n",
       " ('know', 'known'): (('suffix', '', 'n'),\n",
       "  0.45454545454545453,\n",
       "  {('A', 'An'),\n",
       "   ('America', 'American'),\n",
       "   ('give', 'given'),\n",
       "   ('see', 'seen'),\n",
       "   ('take', 'taken')}),\n",
       " ('leader', 'lead'): (('suffix', 'er', ''),\n",
       "  0.5,\n",
       "  {('higher', 'high'),\n",
       "   ('longer', 'long'),\n",
       "   ('lower', 'low'),\n",
       "   ('player', 'play')}),\n",
       " ('leading', 'lead'): (('suffix', 'ing', ''),\n",
       "  0.9,\n",
       "  {('building', 'build'),\n",
       "   ('looking', 'look'),\n",
       "   ('meeting', 'meet'),\n",
       "   ('opening', 'open'),\n",
       "   ('playing', 'play'),\n",
       "   ('saying', 'say'),\n",
       "   ('starting', 'start'),\n",
       "   ('trying', 'try'),\n",
       "   ('working', 'work')}),\n",
       " ('living', 'live'): (('suffix', 'ing', 'e'),\n",
       "  0.875,\n",
       "  {('coming', 'come'),\n",
       "   ('giving', 'give'),\n",
       "   ('having', 'have'),\n",
       "   ('including', 'include'),\n",
       "   ('making', 'make'),\n",
       "   ('taking', 'take'),\n",
       "   ('trading', 'trade')}),\n",
       " ('look', 'looking'): (('suffix', '', 'ing'),\n",
       "  0.9,\n",
       "  {('build', 'building'),\n",
       "   ('lead', 'leading'),\n",
       "   ('meet', 'meeting'),\n",
       "   ('open', 'opening'),\n",
       "   ('play', 'playing'),\n",
       "   ('say', 'saying'),\n",
       "   ('start', 'starting'),\n",
       "   ('try', 'trying'),\n",
       "   ('work', 'working')}),\n",
       " ('makes', 'making'): (('suffix', 'es', 'ing'),\n",
       "  0.8,\n",
       "  {('comes', 'coming'),\n",
       "   ('includes', 'including'),\n",
       "   ('lives', 'living'),\n",
       "   ('takes', 'taking')}),\n",
       " ('need', 'needs'): (('suffix', '', 's'),\n",
       "  0.927536231884058,\n",
       "  {('American', 'Americans'),\n",
       "   ('area', 'areas'),\n",
       "   ('call', 'calls'),\n",
       "   ('case', 'cases'),\n",
       "   ('change', 'changes'),\n",
       "   ('charge', 'charges'),\n",
       "   ('come', 'comes'),\n",
       "   ('comment', 'comments'),\n",
       "   ('cost', 'costs'),\n",
       "   ('day', 'days'),\n",
       "   ('effort', 'efforts'),\n",
       "   ('event', 'events'),\n",
       "   ('force', 'forces'),\n",
       "   ('game', 'games'),\n",
       "   ('goal', 'goals'),\n",
       "   ('group', 'groups'),\n",
       "   ('home', 'homes'),\n",
       "   ('hour', 'hours'),\n",
       "   ('include', 'includes'),\n",
       "   ('issue', 'issues'),\n",
       "   ('job', 'jobs'),\n",
       "   ('leader', 'leaders'),\n",
       "   ('level', 'levels'),\n",
       "   ('live', 'lives'),\n",
       "   ('make', 'makes'),\n",
       "   ('market', 'markets'),\n",
       "   ('member', 'members'),\n",
       "   ('month', 'months'),\n",
       "   ('offer', 'offers'),\n",
       "   ('officer', 'officers'),\n",
       "   ('official', 'officials'),\n",
       "   ('other', 'others'),\n",
       "   ('plan', 'plans'),\n",
       "   ('player', 'players'),\n",
       "   ('point', 'points'),\n",
       "   ('price', 'prices'),\n",
       "   ('problem', 'problems'),\n",
       "   ('product', 'products'),\n",
       "   ('program', 'programs'),\n",
       "   ('project', 'projects'),\n",
       "   ('provide', 'provides'),\n",
       "   ('question', 'questions'),\n",
       "   ('remain', 'remains'),\n",
       "   ('report', 'reports'),\n",
       "   ('result', 'results'),\n",
       "   ('run', 'runs'),\n",
       "   ('sale', 'sales'),\n",
       "   ('say', 'says'),\n",
       "   ('school', 'schools'),\n",
       "   ('service', 'services'),\n",
       "   ('share', 'shares'),\n",
       "   ('show', 'shows'),\n",
       "   ('state', 'states'),\n",
       "   ('student', 'students'),\n",
       "   ('system', 'systems'),\n",
       "   ('take', 'takes'),\n",
       "   ('talk', 'talks'),\n",
       "   ('team', 'teams'),\n",
       "   ('term', 'terms'),\n",
       "   ('thing', 'things'),\n",
       "   ('time', 'times'),\n",
       "   ('want', 'wants'),\n",
       "   ('week', 'weeks'),\n",
       "   ('year', 'years')}),\n",
       " ('needs', 'needed'): (('suffix', 's', 'ed'),\n",
       "  0.8,\n",
       "  {('calls', 'called'),\n",
       "   ('reports', 'reported'),\n",
       "   ('shows', 'showed'),\n",
       "   ('wants', 'wanted')}),\n",
       " ('no', 'No'): (('prefix', 'n', 'N'),\n",
       "  0.8,\n",
       "  {('national', 'National'),\n",
       "   ('new', 'New'),\n",
       "   ('news', 'News'),\n",
       "   ('now', 'Now')}),\n",
       " ('non', 'no'): (('suffix', 'n', ''),\n",
       "  0.5454545454545454,\n",
       "  {('American', 'America'),\n",
       "   ('An', 'A'),\n",
       "   ('given', 'give'),\n",
       "   ('known', 'know'),\n",
       "   ('seen', 'see'),\n",
       "   ('taken', 'take')}),\n",
       " ('play', 'player'): (('suffix', '', 'er'),\n",
       "  0.5,\n",
       "  {('high', 'higher'),\n",
       "   ('lead', 'leader'),\n",
       "   ('long', 'longer'),\n",
       "   ('low', 'lower')}),\n",
       " ('reported', 'reports'): (('suffix', 'ed', 's'),\n",
       "  0.8,\n",
       "  {('called', 'calls'),\n",
       "   ('needed', 'needs'),\n",
       "   ('showed', 'shows'),\n",
       "   ('wanted', 'wants')}),\n",
       " ('she', 'She'): (('prefix', 's', 'S'),\n",
       "  0.8,\n",
       "  {('school', 'School'), ('so', 'So'), ('some', 'Some'), ('state', 'State')}),\n",
       " ('signed', 'sign'): (('suffix', 'ed', ''),\n",
       "  0.875,\n",
       "  {('allowed', 'allow'),\n",
       "   ('called', 'call'),\n",
       "   ('ended', 'end'),\n",
       "   ('expected', 'expect'),\n",
       "   ('helped', 'help'),\n",
       "   ('needed', 'need'),\n",
       "   ('played', 'play'),\n",
       "   ('reached', 'reach'),\n",
       "   ('reported', 'report'),\n",
       "   ('showed', 'show'),\n",
       "   ('started', 'start'),\n",
       "   ('turned', 'turn'),\n",
       "   ('wanted', 'want'),\n",
       "   ('worked', 'work')}),\n",
       " ('systems', 'system'): (('suffix', 's', ''),\n",
       "  0.9420289855072463,\n",
       "  {('Americans', 'American'),\n",
       "   ('areas', 'area'),\n",
       "   ('calls', 'call'),\n",
       "   ('cases', 'case'),\n",
       "   ('changes', 'change'),\n",
       "   ('charges', 'charge'),\n",
       "   ('comes', 'come'),\n",
       "   ('comments', 'comment'),\n",
       "   ('costs', 'cost'),\n",
       "   ('days', 'day'),\n",
       "   ('efforts', 'effort'),\n",
       "   ('events', 'event'),\n",
       "   ('forces', 'force'),\n",
       "   ('games', 'game'),\n",
       "   ('goals', 'goal'),\n",
       "   ('groups', 'group'),\n",
       "   ('homes', 'home'),\n",
       "   ('hours', 'hour'),\n",
       "   ('includes', 'include'),\n",
       "   ('issues', 'issue'),\n",
       "   ('its', 'it'),\n",
       "   ('jobs', 'job'),\n",
       "   ('leaders', 'leader'),\n",
       "   ('levels', 'level'),\n",
       "   ('lives', 'live'),\n",
       "   ('makes', 'make'),\n",
       "   ('markets', 'market'),\n",
       "   ('members', 'member'),\n",
       "   ('months', 'month'),\n",
       "   ('needs', 'need'),\n",
       "   ('offers', 'offer'),\n",
       "   ('officers', 'officer'),\n",
       "   ('officials', 'official'),\n",
       "   ('others', 'other'),\n",
       "   ('plans', 'plan'),\n",
       "   ('players', 'player'),\n",
       "   ('points', 'point'),\n",
       "   ('prices', 'price'),\n",
       "   ('problems', 'problem'),\n",
       "   ('products', 'product'),\n",
       "   ('programs', 'program'),\n",
       "   ('projects', 'project'),\n",
       "   ('provides', 'provide'),\n",
       "   ('questions', 'question'),\n",
       "   ('remains', 'remain'),\n",
       "   ('reports', 'report'),\n",
       "   ('results', 'result'),\n",
       "   ('runs', 'run'),\n",
       "   ('sales', 'sale'),\n",
       "   ('says', 'say'),\n",
       "   ('schools', 'school'),\n",
       "   ('services', 'service'),\n",
       "   ('shares', 'share'),\n",
       "   ('shows', 'show'),\n",
       "   ('states', 'state'),\n",
       "   ('students', 'student'),\n",
       "   ('takes', 'take'),\n",
       "   ('talks', 'talk'),\n",
       "   ('teams', 'team'),\n",
       "   ('terms', 'term'),\n",
       "   ('things', 'thing'),\n",
       "   ('times', 'time'),\n",
       "   ('wants', 'want'),\n",
       "   ('weeks', 'week'),\n",
       "   ('years', 'year')}),\n",
       " ('the', 'The'): (('prefix', 't', 'T'),\n",
       "  0.8571428571428571,\n",
       "  {('that', 'That'),\n",
       "   ('there', 'There'),\n",
       "   ('these', 'These'),\n",
       "   ('they', 'They'),\n",
       "   ('this', 'This'),\n",
       "   ('two', 'Two')}),\n",
       " ('us', 'used'): (('suffix', '', 'ed'),\n",
       "  0.9375,\n",
       "  {('allow', 'allowed'),\n",
       "   ('call', 'called'),\n",
       "   ('end', 'ended'),\n",
       "   ('expect', 'expected'),\n",
       "   ('help', 'helped'),\n",
       "   ('need', 'needed'),\n",
       "   ('play', 'played'),\n",
       "   ('reach', 'reached'),\n",
       "   ('report', 'reported'),\n",
       "   ('show', 'showed'),\n",
       "   ('sign', 'signed'),\n",
       "   ('start', 'started'),\n",
       "   ('turn', 'turned'),\n",
       "   ('want', 'wanted'),\n",
       "   ('work', 'worked')}),\n",
       " ('with', 'With'): (('prefix', 'w', 'W'),\n",
       "  0.8333333333333334,\n",
       "  {('we', 'We'),\n",
       "   ('what', 'What'),\n",
       "   ('when', 'When'),\n",
       "   ('while', 'While'),\n",
       "   ('world', 'World')}),\n",
       " ('worked', 'working'): (('suffix', 'ed', 'ing'),\n",
       "  0.8,\n",
       "  {('followed', 'following'),\n",
       "   ('included', 'including'),\n",
       "   ('played', 'playing'),\n",
       "   ('started', 'starting')})}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphological_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'The') (('prefix', 't', 'T'), 0.8571428571428571, {('they', 'They'), ('two', 'Two'), ('that', 'That'), ('this', 'This'), ('there', 'There'), ('these', 'These')})\n",
      "**************************************************\n",
      "('#-#', '##-#') (('prefix', '', '#'), 0.75, {('##', '###'), ('#,###', '##,###'), ('#:##', '##:##'), ('#.##', '##.##'), ('##,###', '###,###'), ('#.#', '##.#')})\n",
      "**************************************************\n",
      "('This', 'this') (('prefix', 'T', 't'), 0.8571428571428571, {('They', 'they'), ('These', 'these'), ('Two', 'two'), ('There', 'there'), ('The', 'the'), ('That', 'that')})\n",
      "**************************************************\n",
      "('with', 'With') (('prefix', 'w', 'W'), 0.8333333333333334, {('world', 'World'), ('while', 'While'), ('we', 'We'), ('when', 'When'), ('what', 'What')})\n",
      "**************************************************\n",
      "('know', 'known') (('suffix', '', 'n'), 0.45454545454545453, {('take', 'taken'), ('A', 'An'), ('give', 'given'), ('see', 'seen'), ('America', 'American')})\n",
      "**************************************************\n",
      "('according', 'According') (('prefix', 'a', 'A'), 0.8333333333333334, {('all', 'All'), ('as', 'As'), ('at', 'At'), ('an', 'An'), ('after', 'After')})\n",
      "**************************************************\n",
      "('need', 'needs') (('suffix', '', 's'), 0.927536231884058, {('report', 'reports'), ('team', 'teams'), ('system', 'systems'), ('force', 'forces'), ('term', 'terms'), ('effort', 'efforts'), ('case', 'cases'), ('year', 'years'), ('job', 'jobs'), ('comment', 'comments'), ('service', 'services'), ('live', 'lives'), ('say', 'says'), ('question', 'questions'), ('talk', 'talks'), ('school', 'schools'), ('run', 'runs'), ('share', 'shares'), ('want', 'wants'), ('day', 'days'), ('plan', 'plans'), ('provide', 'provides'), ('official', 'officials'), ('show', 'shows'), ('event', 'events'), ('officer', 'officers'), ('include', 'includes'), ('group', 'groups'), ('come', 'comes'), ('level', 'levels'), ('American', 'Americans'), ('issue', 'issues'), ('time', 'times'), ('month', 'months'), ('goal', 'goals'), ('make', 'makes'), ('sale', 'sales'), ('change', 'changes'), ('problem', 'problems'), ('program', 'programs'), ('charge', 'charges'), ('state', 'states'), ('product', 'products'), ('cost', 'costs'), ('hour', 'hours'), ('game', 'games'), ('take', 'takes'), ('offer', 'offers'), ('other', 'others'), ('result', 'results'), ('market', 'markets'), ('week', 'weeks'), ('remain', 'remains'), ('member', 'members'), ('area', 'areas'), ('point', 'points'), ('home', 'homes'), ('player', 'players'), ('leader', 'leaders'), ('project', 'projects'), ('thing', 'things'), ('student', 'students'), ('price', 'prices'), ('call', 'calls')})\n",
      "**************************************************\n",
      "('give', 'giving') (('suffix', 'e', 'ing'), 0.875, {('come', 'coming'), ('trade', 'trading'), ('have', 'having'), ('include', 'including'), ('take', 'taking'), ('live', 'living'), ('make', 'making')})\n",
      "**************************************************\n",
      "('####', '###') (('prefix', '#', ''), 0.875, {('###', '##'), ('##-#', '#-#'), ('##,###', '#,###'), ('##.##', '#.##'), ('###,###', '##,###'), ('##:##', '#:##'), ('##.#', '#.#')})\n",
      "**************************************************\n",
      "('systems', 'system') (('suffix', 's', ''), 0.9420289855072463, {('takes', 'take'), ('results', 'result'), ('changes', 'change'), ('forces', 'force'), ('terms', 'term'), ('areas', 'area'), ('things', 'thing'), ('sales', 'sale'), ('provides', 'provide'), ('wants', 'want'), ('schools', 'school'), ('plans', 'plan'), ('talks', 'talk'), ('students', 'student'), ('cases', 'case'), ('needs', 'need'), ('charges', 'charge'), ('lives', 'live'), ('homes', 'home'), ('markets', 'market'), ('members', 'member'), ('costs', 'cost'), ('calls', 'call'), ('includes', 'include'), ('Americans', 'American'), ('says', 'say'), ('shares', 'share'), ('goals', 'goal'), ('times', 'time'), ('issues', 'issue'), ('projects', 'project'), ('prices', 'price'), ('makes', 'make'), ('efforts', 'effort'), ('days', 'day'), ('comes', 'come'), ('runs', 'run'), ('problems', 'problem'), ('games', 'game'), ('years', 'year'), ('comments', 'comment'), ('others', 'other'), ('teams', 'team'), ('events', 'event'), ('leaders', 'leader'), ('states', 'state'), ('its', 'it'), ('shows', 'show'), ('remains', 'remain'), ('products', 'product'), ('hours', 'hour'), ('questions', 'question'), ('levels', 'level'), ('reports', 'report'), ('offers', 'offer'), ('points', 'point'), ('jobs', 'job'), ('weeks', 'week'), ('programs', 'program'), ('officials', 'official'), ('months', 'month'), ('groups', 'group'), ('players', 'player'), ('officers', 'officer'), ('services', 'service')})\n",
      "**************************************************\n",
      "('When', 'when') (('prefix', 'W', 'w'), 0.8333333333333334, {('World', 'world'), ('With', 'with'), ('What', 'what'), ('We', 'we'), ('While', 'while')})\n",
      "**************************************************\n",
      "('no', 'No') (('prefix', 'n', 'N'), 0.8, {('now', 'Now'), ('news', 'News'), ('new', 'New'), ('national', 'National')})\n",
      "**************************************************\n",
      "('non', 'no') (('suffix', 'n', ''), 0.5454545454545454, {('taken', 'take'), ('known', 'know'), ('An', 'A'), ('given', 'give'), ('seen', 'see'), ('American', 'America')})\n",
      "**************************************************\n",
      "('she', 'She') (('prefix', 's', 'S'), 0.8, {('state', 'State'), ('school', 'School'), ('so', 'So'), ('some', 'Some')})\n",
      "**************************************************\n",
      "('play', 'player') (('suffix', '', 'er'), 0.5, {('lead', 'leader'), ('long', 'longer'), ('low', 'lower'), ('high', 'higher')})\n",
      "**************************************************\n",
      "('look', 'looking') (('suffix', '', 'ing'), 0.9, {('build', 'building'), ('say', 'saying'), ('meet', 'meeting'), ('open', 'opening'), ('try', 'trying'), ('work', 'working'), ('lead', 'leading'), ('start', 'starting'), ('play', 'playing')})\n",
      "**************************************************\n",
      "('us', 'used') (('suffix', '', 'ed'), 0.9375, {('work', 'worked'), ('turn', 'turned'), ('allow', 'allowed'), ('report', 'reported'), ('want', 'wanted'), ('reach', 'reached'), ('play', 'played'), ('call', 'called'), ('help', 'helped'), ('expect', 'expected'), ('sign', 'signed'), ('show', 'showed'), ('need', 'needed'), ('start', 'started'), ('end', 'ended')})\n",
      "**************************************************\n",
      "('living', 'live') (('suffix', 'ing', 'e'), 0.875, {('giving', 'give'), ('coming', 'come'), ('having', 'have'), ('trading', 'trade'), ('including', 'include'), ('making', 'make'), ('taking', 'take')})\n",
      "**************************************************\n",
      "('including', 'includes') (('suffix', 'ing', 'es'), 0.8, {('taking', 'takes'), ('making', 'makes'), ('coming', 'comes'), ('living', 'lives')})\n",
      "**************************************************\n",
      "('including', 'included') (('suffix', 'ing', 'ed'), 0.8, {('working', 'worked'), ('playing', 'played'), ('following', 'followed'), ('starting', 'started')})\n",
      "**************************************************\n",
      "('based', 'base') (('suffix', 'd', ''), 0.8461538461538461, {('continued', 'continue'), ('included', 'include'), ('used', 'use'), ('received', 'receive'), ('increased', 'increase'), ('provided', 'provide'), ('closed', 'close'), ('released', 'release'), ('named', 'name'), ('charged', 'charge'), ('moved', 'move')})\n",
      "**************************************************\n",
      "('base', 'based') (('suffix', '', 'd'), 0.8461538461538461, {('increase', 'increased'), ('name', 'named'), ('continue', 'continued'), ('charge', 'charged'), ('provide', 'provided'), ('close', 'closed'), ('release', 'released'), ('receive', 'received'), ('use', 'used'), ('move', 'moved'), ('include', 'included')})\n",
      "**************************************************\n",
      "('She', 'she') (('prefix', 'S', 's'), 0.8, {('School', 'school'), ('Some', 'some'), ('So', 'so'), ('State', 'state')})\n",
      "**************************************************\n",
      "('According', 'according') (('prefix', 'A', 'a'), 0.8333333333333334, {('At', 'at'), ('As', 'as'), ('After', 'after'), ('All', 'all'), ('An', 'an')})\n",
      "**************************************************\n",
      "('signed', 'sign') (('suffix', 'ed', ''), 0.875, {('ended', 'end'), ('showed', 'show'), ('allowed', 'allow'), ('turned', 'turn'), ('played', 'play'), ('helped', 'help'), ('worked', 'work'), ('reported', 'report'), ('needed', 'need'), ('started', 'start'), ('called', 'call'), ('wanted', 'want'), ('expected', 'expect'), ('reached', 'reach')})\n",
      "**************************************************\n",
      "('leader', 'lead') (('suffix', 'er', ''), 0.5, {('higher', 'high'), ('player', 'play'), ('longer', 'long'), ('lower', 'low')})\n",
      "**************************************************\n",
      "('reported', 'reports') (('suffix', 'ed', 's'), 0.8, {('showed', 'shows'), ('needed', 'needs'), ('called', 'calls'), ('wanted', 'wants')})\n",
      "**************************************************\n",
      "('leading', 'lead') (('suffix', 'ing', ''), 0.9, {('looking', 'look'), ('starting', 'start'), ('trying', 'try'), ('meeting', 'meet'), ('playing', 'play'), ('building', 'build'), ('working', 'work'), ('saying', 'say'), ('opening', 'open')})\n",
      "**************************************************\n",
      "('worked', 'working') (('suffix', 'ed', 'ing'), 0.8, {('followed', 'following'), ('started', 'starting'), ('included', 'including'), ('played', 'playing')})\n",
      "**************************************************\n",
      "('News', 'news') (('prefix', 'N', 'n'), 0.8, {('No', 'no'), ('National', 'national'), ('Now', 'now'), ('New', 'new')})\n",
      "**************************************************\n",
      "('needs', 'needed') (('suffix', 's', 'ed'), 0.8, {('calls', 'called'), ('shows', 'showed'), ('reports', 'reported'), ('wants', 'wanted')})\n",
      "**************************************************\n",
      "('makes', 'making') (('suffix', 'es', 'ing'), 0.8, {('takes', 'taking'), ('comes', 'coming'), ('includes', 'including'), ('lives', 'living')})\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for item in morphological_rules:\n",
    "    print (item, morphological_rules[item])\n",
    "    print (\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'The')\n"
     ]
    }
   ],
   "source": [
    "for rule in morphological_rules:\n",
    "    print (rule)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists('../data/rules_'+ str(len(word_vectors.vocab))):\n",
    "#     rule_file_r = open('../data/rules_'+ str(len(word_vectors.vocab)), 'rb')\n",
    "#     morphological_rules = pickle.load(rule_file_r)\n",
    "# else:\n",
    "#     hit_rates = calculate_hit_rate(sampled_patterns, pair_wise_similarity)\n",
    "#     hit_rate_file_w = open('../data/rules_'+ str(len(word_vectors.vocab)),\"wb\" )\n",
    "#     pickle.dump(hit_rates, hit_rate_file_w)\n",
    "#     hit_rate_file_w.close()\n",
    "# # %time hit_rates = calculate_hit_rate(sampled_patterns, pair_wise_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 3.72 ms\n"
     ]
    }
   ],
   "source": [
    "%time G.add_nodes_from(word_vectors.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_graph(G):\n",
    "    for dw,support in morphological_rules.items():\n",
    "        morp_rule, hit_rate,support_set = support\n",
    "        (word1, word2) = dw\n",
    "        for (word3, word4) in support_set:\n",
    "            (rank,cos_sim) = get_similarity_rank((word1,word2),(word3,word4))\n",
    "            if rank < 3 and cos_sim > 0.5:\n",
    "                G.add_edge(word3,word4,dw=dw,cos=0.35,rank=1)\n",
    "                if not G.has_edge(word3,word4,key=dw):\n",
    "                    G.add_edge(word3,word4,key=dw,cos=cos_sim,rank=rank)\n",
    "            else:\n",
    "    #             print (rank,cos_sim, word3, word2, dw)\n",
    "                pass\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makes\n",
      "{0: {'dw': ('need', 'needs'), 'cos': 0.35, 'rank': 1}, ('need', 'needs'): {'cos': 0.5872986912727356, 'rank': 1}}\n",
      "making\n",
      "{0: {'dw': ('give', 'giving'), 'cos': 0.35, 'rank': 1}, ('give', 'giving'): {'cos': 0.9000281095504761, 'rank': 0}}\n",
      "[('make', 'makes', {'dw': ('need', 'needs'), 'cos': 0.35, 'rank': 1}), ('make', 'makes', {'cos': 0.5872986912727356, 'rank': 1}), ('make', 'making', {'dw': ('give', 'giving'), 'cos': 0.35, 'rank': 1}), ('make', 'making', {'cos': 0.9000281095504761, 'rank': 0})]\n",
      "[('make', 'makes', 0), ('make', 'makes', ('need', 'needs')), ('make', 'making', 0), ('make', 'making', ('give', 'giving'))]\n"
     ]
    }
   ],
   "source": [
    "for item in G['make']:\n",
    "    print (item)\n",
    "    d = G['make'][item]\n",
    "    print(d)\n",
    "print (G.out_edges('make',data=True))\n",
    "print (G.out_edges('make',keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print (len(G.edges))\n",
    "print (len(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in G.nodes:\n",
    "#     if G.out_degree(item) > 1:\n",
    "#         print (item, G.out_degree(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_graph(G):\n",
    "    for node in list(G.nodes):\n",
    "        for neighbor in list(G.neighbors(node)):\n",
    "            if word_vectors.vocab[node].count > word_vectors.vocab[neighbor].count:\n",
    "                if G.has_edge(node, neighbor):\n",
    "                    G.remove_edges_from(set(G.in_edges(neighbor,keys=True)) and set(G.out_edges(node,keys=True)))\n",
    "                if G.number_of_edges(neighbor, node) > 1:\n",
    "    #                 print (list(G.in_edges(node,keys=True)))\n",
    "                    n_list = [(G[neighbor][node][item]['rank'], G[neighbor][node][item]['cos'], item) for item in (G[neighbor][node].keys())]\n",
    "                    min_rank_edge = min(n_list,key=itemgetter(0))\n",
    "                    max_cos_edge = max(n_list,key=itemgetter(1))\n",
    "    #                 print (list(G.in_edges(node,keys=True)))\n",
    "                    remove_edges = [x for x in list(G.in_edges(node,keys=True)) if x != (neighbor,node,min_rank_edge[2])]\n",
    "    #                 print (remove_edges)\n",
    "                    G.remove_edges_from(remove_edges)\n",
    "                    if G.number_of_edges(neighbor, node) > 1:\n",
    "                        remove_edges = [x for x in list(G.in_edges(node,keys=True)) if x != (neighbor,node,max_cos_edge[2])]\n",
    "                        G.remove_edges_from(remove_edges)\n",
    "                        print (list(G.in_edges(node,keys=True)))\n",
    "            else:\n",
    "                if G.has_edge(neighbor, node):\n",
    "                    G.remove_edges_from(set(G.in_edges(node,keys=True)) and set(G.out_edges(neighbor,keys=True)))\n",
    "                if G.number_of_edges(node, neighbor) > 1:\n",
    "                    n_list = [(G[node][neighbor][item]['rank'], G[node][neighbor][item]['cos'], item) for item in (G[node][neighbor].keys())]\n",
    "                    min_rank_edge = min(n_list,key=itemgetter(0))\n",
    "                    max_cos_edge = max(n_list,key=itemgetter(1))\n",
    "                    remove_edges = [x for x in list(G.in_edges(neighbor,keys=True)) if x != (node,neighbor,min_rank_edge[2])]\n",
    "                    G.remove_edges_from(remove_edges)\n",
    "                    if G.number_of_edges(node, neighbor) > 1:\n",
    "                        remove_edges = [x for x in list(G.in_edges(neighbor,keys=True)) if x != (node,neighbor,max_cos_edge[2])]\n",
    "                        G.remove_edges_from(remove_edges)\n",
    "                        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = normalize_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print (len(G.edges))\n",
    "print (len(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
